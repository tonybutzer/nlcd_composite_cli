{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726d4f8a-9f35-4886-8463-c192dae02450",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55ba9f9-ed68-4af1-9866-cb45370b7870",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'mean' from 'statistics' (/data/opt/nlcd_compositing/statistics.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrasterio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrio\u001b[39;00m\n",
      "File \u001b[0;32m/data/mambaforge/envs/ts2024/lib/python3.11/site-packages/geopandas/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m options\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeoseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeoSeries\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeodataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeoDataFrame\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m points_from_xy\n",
      "File \u001b[0;32m/data/mambaforge/envs/ts2024/lib/python3.11/site-packages/geopandas/geoseries.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeoPandasBase, _delegate_property\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_series\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _explore_geoseries\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _compat \u001b[38;5;28;01mas\u001b[39;00m compat\n",
      "File \u001b[0;32m/data/mambaforge/envs/ts2024/lib/python3.11/site-packages/geopandas/explore.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatistics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LineString\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'mean' from 'statistics' (/data/opt/nlcd_compositing/statistics.py)"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "import logging\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import geopandas as gp\n",
    "\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from rasterio.windows import Window\n",
    "import fsspec\n",
    "import boto3\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lcnext import lsc2ard as c2ard\n",
    "from lcnext.lsc2ard import LandsatARDObservation\n",
    "from lcnext.static import LANDSAT_SENSORS\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     level=logging.ERROR,\n",
    "#     format=\"%(asctime)s %(message)s\",\n",
    "#     datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "#     stream=sys.stderr,\n",
    "# )\n",
    "\n",
    "log = logging.getLogger()\n",
    "\n",
    "\n",
    "def sr_bandnumbers(sensor: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    SR band numbers for the given sensor\n",
    "    oli-tirs, tm, etm\n",
    "    \"\"\"\n",
    "    if sensor == 'oli-tirs':\n",
    "        return [2, 3, 4, 5, 6, 7]\n",
    "    elif (sensor == 'tm') | (sensor == 'etm'):\n",
    "        return [1, 2, 3, 4, 5, 7]\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "def qa_std_layers(deets: LandsatARDObservation) -> List[str]:\n",
    "    \"\"\"\n",
    "    Standard list of QA bands associated with the given observation\n",
    "    \"\"\"\n",
    "    return [f'{deets.root_id}_QA_PIXEL.TIF']\n",
    "\n",
    "\n",
    "def sr_std_layers(deets: LandsatARDObservation) -> List[str]:\n",
    "    \"\"\"\n",
    "    Standard list of needed bands associated with the given observation\n",
    "    \"\"\"\n",
    "    return [f'{deets.root_id}_SR_B{b}.TIF'\n",
    "            for b in sr_bandnumbers(deets.sensor)]\n",
    "    \n",
    "def dstack_idx(idxs: np.ndarray):\n",
    "    \"\"\"\n",
    "    Takes 2d index returns from numpy.argmin or numpy.argmax on a 3d array where axis=0 and turns it into\n",
    "    a tuple of tuples for indexing back into the 3d array\n",
    "    \"\"\"\n",
    "    rows, cols = idxs.shape\n",
    "    \n",
    "    d_stack_idx = (idxs,\n",
    "            np.repeat(np.arange(rows).reshape(-1, 1), repeats=rows, axis=1),\n",
    "            np.repeat(np.arange(cols).reshape(1, -1), repeats=cols, axis=0))\n",
    "    \n",
    "    # print(f'dstack input idx: {idxs}')\n",
    "    \n",
    "    return d_stack_idx\n",
    "\n",
    "\n",
    "def difference_absolute(arr1: np.ndarray, arr2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the absolute distance between the values in the two different arrays\n",
    "    \"\"\"\n",
    "    diff_abs = np.abs(difference(arr1, arr2))\n",
    "    \n",
    "    return diff_abs\n",
    "\n",
    "\n",
    "def difference(arr1: np.ndarray, arr2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Difference the two given arrays\n",
    "    \"\"\" \n",
    "    return arr1 - arr2\n",
    "\n",
    "\n",
    "def difference_median(arr):\n",
    "    \"\"\"\n",
    "    Calculate the absolute difference each value is from the median\n",
    "    \"\"\"\n",
    "    \n",
    "    median = np.ma.median(arr, axis=0)\n",
    "    # print(f'median: {median}')\n",
    "    \n",
    "    diff_median = difference_absolute(arr, median)\n",
    "    # print(f'diff_median: {diff_median}')\n",
    "    \n",
    "    return diff_median\n",
    "\n",
    "\n",
    "def sum_squares(arrs):\n",
    "    \"\"\"\n",
    "    Square and then sum all the values (element wise) in the given arrays\n",
    "    \"\"\"\n",
    "    sum_sqs = np.ma.sum([np.ma.power(a, 2) for a in arrs], axis=0)\n",
    "    \n",
    "    # print(f'sum_sqs: {sum_sqs}')\n",
    "    \n",
    "    return sum_sqs\n",
    "\n",
    "\n",
    "def distance_overall(spectral):\n",
    "    \"\"\"\n",
    "    Return the euclidean distance for observations that come closest to the overall median value. Where spectral \n",
    "    is the input 6 band array. The shape of the input spectral band is (#of bands, #of ARD observations/scenes, Xsize of Window, Ysize of Window).\n",
    "    The \n",
    "    \"\"\"\n",
    "\n",
    "    euc_dist = np.ma.sqrt(sum_squares([difference_median(spectral[b])\n",
    "                                       for b in range(spectral.shape[0])]))\n",
    "    \n",
    "    \n",
    "    # print(f'euc_dist: {euc_dist}')\n",
    "    \n",
    "    idxs = dstack_idx(np.ma.argmin(euc_dist, axis=0))\n",
    "    \n",
    "    # print(f'idxs: {idxs}')\n",
    "     \n",
    "    dist_overall = np.array([spectral[b][idxs]\n",
    "                     for b in range(spectral.shape[0])])\n",
    "    \n",
    "    \n",
    "    return dist_overall\n",
    "    \n",
    "\n",
    "# def std_mask(qa_arr: np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Standard mask for compositing stuff\n",
    "#     \"\"\"\n",
    "    \n",
    "#     std_masks = (c2ard.qa_fill(qa_arr) |\n",
    "#             c2ard.qa_cirrus(qa_arr) |\n",
    "#             c2ard.qa_cloud(qa_arr) |\n",
    "#             c2ard.qa_cl_shadow(qa_arr) |\n",
    "#             c2ard.qa_snow(qa_arr))\n",
    "    \n",
    "#     return std_masks\n",
    "\n",
    "\n",
    "def create_qa_arrays(qa_arr: np.array):\n",
    "    '''\n",
    "    Create a dictionary of QA arrays.\n",
    "    '''\n",
    "    qa_arrays = {\n",
    "        'qa_fill': c2ard.qa_fill(qa_arr),\n",
    "        'qa_cirrus': c2ard.qa_cirrus(qa_arr),\n",
    "        'qa_cloud': c2ard.qa_cloud(qa_arr),\n",
    "        'qa_cl_shadow': c2ard.qa_cl_shadow(qa_arr),\n",
    "        'qa_snow': c2ard.qa_snow(qa_arr)\n",
    "    }\n",
    "    return qa_arrays\n",
    "\n",
    "\n",
    "\n",
    "def get_qa_count(qa_arr: np.ndarray)-> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function calculates the QA count i.e. the number of TRUE values per pixel location across all input ARDS.\n",
    "    The input qa_arr is converted to a boolean_arrays which is a python list the size of the number of QA filters \n",
    "    used in the processing.In this function we first convert the boolean_array TRUE/FALSE list of arrays to a binary\n",
    "    list of arrays (1's' and 0's') then sums across each pixel to geta the count of 1's/TRUE's per QA filter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    boolean_arrays : np.ndarray\n",
    "        DESCRIPTION.\n",
    "        Example boolean_array = [array([[[False,  True],\n",
    "                 [ True,  True]],\n",
    "         \n",
    "                [[False,  True],\n",
    "                 [ True, False]],\n",
    "         \n",
    "                [[False, False],\n",
    "                 [ True, False]]]),\n",
    "                 \n",
    "         array([[[False, False],\n",
    "                 [False, False]],\n",
    "         \n",
    "                [[False, False],\n",
    "                 [False, False]],\n",
    "         \n",
    "                [[ True,  True],\n",
    "                 [ True,  True]]])]\n",
    "        \n",
    "        Which is a list of 2 arrays (represents 2 QA filters) each with 3 sub arrays (boolean data for a specific \n",
    "        window in 3 ARD tiles) at 2 by 2 size. After converting all Trues to 1's and False to 0's the result \n",
    "        qa_count array for this boolean is\n",
    "                [array([[0, 2],\n",
    "                        [3, 1]]),\n",
    "                 array([[1, 1],\n",
    "                        [1, 1]])]\n",
    "        In the first QA filter (first array of result) the first pixel value of 0 shows that for that pixel location\n",
    "        there were 0 True values across all 3 ARD's used, likewise the value of 2 shows that for that pixel location \n",
    "        2 of 3 ARD's for this pixel location is True\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    # print(f'qa_arr: {qa_arr}')\n",
    "    qa_arrays = create_qa_arrays(qa_arr)\n",
    "    \n",
    "    # Extract the boolean arrays from the dictionary\n",
    "    boolean_arrays = list(qa_arrays.values())\n",
    "    # print(f'boolean_arrays: {boolean_arrays}')\n",
    "    \n",
    "    # Convert True to 1 and False to 0 for each array in the list\n",
    "    binary_arrays = [arr1.astype(int) for arr1 in boolean_arrays]\n",
    "    \n",
    "    # print(f'binary_arrays: {binary_arrays}')\n",
    "    \n",
    "    # Sum along the 0-axis for each array in the list\n",
    "    qa_count_arrays = [np.sum(arr2, axis=0) for arr2 in binary_arrays]\n",
    "    \n",
    "    # print(f'qa_count: {qa_count_arrays}')\n",
    "    \n",
    "    return qa_count_arrays\n",
    "\n",
    "\n",
    "def get_band_ids(bandcombotype: str, sensor: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the band Ids for the specified combination of bands given the sensor\n",
    "    https://www.usgs.gov/media/images/common-landsat-band-combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    match bandcombotype:\n",
    "        case 'rgb':\n",
    "            band_ids = {\n",
    "            'oli-tirs': ['B4', 'B3', 'B2'],\n",
    "            'tm': ['B3', 'B2', 'B1'],\n",
    "            'etm': ['B3', 'B2', 'B1']\n",
    "             }\n",
    "        case 'vegetation':\n",
    "            band_ids = {\n",
    "            'oli-tirs': ['B6', 'B5', 'B4'],\n",
    "            'tm': ['B5', 'B4', 'B3'],\n",
    "            'etm': ['B5', 'B4', 'B3']\n",
    "            }\n",
    "        case _:\n",
    "            raise ValueError(f'Platform not recognized: {platform}')\n",
    "\n",
    "    return band_ids.get(sensor.lower(), None)\n",
    "\n",
    "\n",
    "def std_mask(qa_arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Standard mask for each observation used in compositing, returns pixel locations with TRUE or FALSE for masking\n",
    "    \"\"\"\n",
    "    # print(f'qa_arr: {qa_arr}')\n",
    "    qa_arrays = create_qa_arrays(qa_arr)\n",
    "    \n",
    "    # Extract the boolean arrays from the dictionary\n",
    "    boolean_arrays = list(qa_arrays.values())\n",
    "    \n",
    "    # Combine the boolean arrays using bitwise OR\n",
    "    std_masks = np.bitwise_or.reduce(boolean_arrays)\n",
    "    \n",
    "    # print(f'std_masks: {std_masks}')\n",
    "    \n",
    "    return std_masks    \n",
    "    \n",
    "\n",
    "\n",
    "def get_cog_metadata(window: Window, ids: List[LandsatARDObservation]):\n",
    "    \"\"\"\n",
    "    Pulls in a single ARD tile and returns the crs, transform, height, width, dtype,\n",
    "    \"\"\"\n",
    "    env = rio_env()\n",
    "    obs_layer = sr_std_layers(ids[0])[0] #select a single layer to retrieve metadata from\n",
    "    path = '/'.join([ids[0].std_path, obs_layer])\n",
    "    \n",
    "    try:\n",
    "        with env:\n",
    "            with rio.open('s3://' + path) as ds:\n",
    "                metadata_list = []\n",
    "\n",
    "                window_info = {\n",
    "                    'dtype': ds.dtypes[0],  # Assuming index 1\n",
    "                    'transform': ds.window_transform(window),\n",
    "                    'crs': ds.crs,\n",
    "                    'width': window.width,\n",
    "                    'height': window.height,\n",
    "                    'count': ds.count\n",
    "                }\n",
    "\n",
    "                metadata_list.append(window_info)\n",
    "\n",
    "                log.info(f\"Metadata for window {window}: {window_info}\\n\\n\")\n",
    "\n",
    "                return metadata_list\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing raster: {path}, {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "# def read_raster(path: str, fs: fsspec.AbstractFileSystem, env: rio.env, index=1, window=None, boundless=False, fill_value=None):\n",
    "#     try:\n",
    "#         with fs.open(path) as f:\n",
    "#             with env:\n",
    "#                 with rio.open(f) as ds:\n",
    "#                     return ds.read(index, window=window, boundless=boundless, fill_value=fill_value)\n",
    "#     except:\n",
    "#         print(path)\n",
    "#         raise\n",
    "\n",
    "\n",
    "def read_raster_pure_rio(path: str, env: rio.env, index=1, window=None, boundless=False, fill_value=None):\n",
    "    try:\n",
    "        with env:\n",
    "            with rio.open('s3://' + path) as ds:\n",
    "\n",
    "                return ds.read(index, window=window, boundless=boundless, fill_value=fill_value)\n",
    "    except:\n",
    "        print(path)\n",
    "        raise\n",
    "\n",
    "\n",
    "        \n",
    "# def comp_worker(window: Window, ids: List[LandsatARDObservation], storage_options: dict) -> np.ndarray:\n",
    "#     log.info(f'Processing {window}')\n",
    "#     fs = fsspec.filesystem(\"s3\", **storage_options)\n",
    "#     env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR')\n",
    "\n",
    "#     big_arr = np.zeros(shape=(6, len(ids), window.height, window.width), dtype=int)\n",
    "#     qas = np.zeros(shape=(len(ids), window.height, window.width), dtype=int)\n",
    "\n",
    "#     for idx1, d in enumerate(ids):\n",
    "#         log.info(f'Pulling data for {d}')\n",
    "#         layers = sr_std_layers(d)\n",
    "#         qlayer = qa_std_layers(d)[0]\n",
    "        \n",
    "#         for idx2, b in enumerate(layers):\n",
    "#             big_arr[idx2, idx1] = read_raster('/'.join([d.std_path, b]), fs, env, window=window, boundless=False)\n",
    "\n",
    "#         qas[idx1] = read_raster('/'.join([d.std_path, qlayer]), fs, env, window=window, boundless=False)\n",
    "        \n",
    "#     return window, distance_overall(np.ma.masked_array(big_arr,\n",
    "#                                                dtype=big_arr.dtype,\n",
    "#                                                mask=np.repeat(np.expand_dims(std_mask(qas), axis=0),\n",
    "#                                                               repeats=6,\n",
    "#                                                               axis=0)))\n",
    "\n",
    "def sort_percentiles(rgb_arr, percent_cut_off):\n",
    "    \"\"\"\n",
    "    Use in cases where valid pixels are being removed due to snow or terrain shadows \n",
    "    when pixel_qa is applied. This function works by summing the red, green and blue\n",
    "    arrays for all observations. It then sorts which pixels to keep using a upper and\n",
    "    lower percentile threshold. I.e. each pixel location is given a TRUE if it falls \n",
    "    outside of the upper and lower threshold otherwise FALSE if its being kept.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sum the red green and blue bands\n",
    "    sum_rgb = np.sum(rgb_arr, axis=1)\n",
    "\n",
    "    # Calculate the lower and upper percentiles for each pixel location\n",
    "    lower_percentiles = np.percentile(sum_rgb, percent_cut_off, axis=0)\n",
    "    upper_percentiles = np.percentile(sum_rgb, 100-percent_cut_off, axis=0)\n",
    "\n",
    "    # Find pixels outside the lower or higher 10% cut for each pixel\n",
    "    sorted_pixels = np.logical_or(sum_rgb <= lower_percentiles, sum_rgb  >=  upper_percentiles)\n",
    "\n",
    "    # # Print the result\n",
    "    # print(\"3D Array:\")\n",
    "    # print(sum_rgb)\n",
    "    # print(\"\\nPixels oustide lower and higher 10% cut:\")\n",
    "    # print(sorted_pixels)\n",
    "    \n",
    "    return sorted_pixels        \n",
    "\n",
    "\n",
    "def comp_worker_pure_rio(window: Window, ids: List[LandsatARDObservation], *args, **kwargs) -> np.ndarray:\n",
    "    log.info(f'PROCESSING {window}')\n",
    "    env = rio_env()\n",
    "    \n",
    "    big_arr = np.zeros(shape=(6, len(ids), window.height, window.width), dtype=int)\n",
    "    rgb_arr = np.zeros(shape=(len(ids),3, window.height, window.width), dtype=int)\n",
    "    qas = np.zeros(shape=(len(ids), window.height, window.width), dtype=int)\n",
    "    \n",
    "    for idx1, d in enumerate(ids):\n",
    "        log.info(f'Pulling data for {d}')\n",
    "        layers = sr_std_layers(d)\n",
    "        qlayer = qa_std_layers(d)[0]\n",
    "        \n",
    "        qas[idx1] = read_raster_pure_rio('/'.join([d.std_path, qlayer]), env, window=window, boundless=True)\n",
    "        # print(f'qas[idx1]: {qas}')\n",
    "        \n",
    "        band_ids = get_band_ids('rgb', d.sensor)\n",
    "        # print(f'>>> band_ids: {band_ids}')\n",
    "        \n",
    "        for idx2, b in enumerate(layers):\n",
    "            for idx3, band_id in enumerate(band_ids):\n",
    "                if band_id in b:\n",
    "                    # print(f'>>> b: {b}')\n",
    "                    rgb_arr[idx1, idx3] = read_raster_pure_rio('/'.join([d.std_path, b]), env, window=window, boundless=True)\n",
    "            # print(f'>>> rgb_arr: {rgb_arr}')\n",
    "                \n",
    "            big_arr[idx2, idx1] = read_raster_pure_rio('/'.join([d.std_path, b]), env, window=window, boundless=True)\n",
    "                   \n",
    "        # print(f'big_arr: {big_arr}')             \n",
    "  \n",
    "    # masked_spectral_array = np.ma.masked_array(big_arr,\n",
    "    #                                            dtype=big_arr.dtype,\n",
    "    #                                            mask=np.repeat(np.expand_dims(std_mask(qas), axis=0),\n",
    "    #                                                           repeats=6,\n",
    "    #                                                           axis=0))\n",
    " \n",
    "    masked_spectral_array = np.ma.masked_array(big_arr,\n",
    "                                               dtype=big_arr.dtype,\n",
    "                                               mask=np.repeat(np.expand_dims(sort_percentiles(rgb_arr,10), axis=0),\n",
    "                                                              repeats=6,\n",
    "                                                              axis=0))\n",
    "    # print(f'masked_spectral_array: {masked_spectral_array}')\n",
    "    dist_overall = distance_overall(masked_spectral_array)  \n",
    "    # log.info(f' DISTANCE_OVERALL: {np.count_nonzero(dist_overall)} filled pixels of {dist_overall.size}')\n",
    "    # print(f'dist_overall: {dist_overall}')\n",
    "    qa_count = get_qa_count(qas) \n",
    "    \n",
    "    return window, dist_overall, qa_count\n",
    "\n",
    "\n",
    "def year_deets(fs: fsspec.AbstractFileSystem, year: int, sensor: str, region: str, horiz: int, vert: int) -> List[LandsatARDObservation]:\n",
    "    \"\"\"\n",
    "    Build out the details for each observation for a given year/sensor\n",
    "    \"\"\"\n",
    "    return [c2ard.obs_deets(p)\n",
    "            for p in\n",
    "            fs.ls(f'usgs-landsat-ard/collection02/{sensor}/{year}/{region}/{horiz:03}/{vert:03}') if not p.endswith('.json')] \n",
    "                                                                                ##since each dir has a JSON file causing errors\n",
    "\n",
    "def find_observations(fs: fsspec.AbstractFileSystem, start_date: dt.datetime.date, end_date: dt.datetime.date, sensor: str, region: str, horiz: int, vert: int):\n",
    "    \"\"\"\n",
    "    Find all observations to fit within the specified start_date/end_date for a given sensor\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for year in range(start_date.year, end_date.year + 1):\n",
    "        ret.extend(filter(lambda x: start_date <= x.acquired <= end_date, year_deets(fs, year, sensor, region, horiz, vert)))\n",
    "    # print(ret)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def check_sensor_availability(sensor_name: str, year: int) -> str:\n",
    "    \"\"\"\n",
    "    Determine sensor availability\n",
    "    \"\"\"\n",
    "    start_year, end_year = {\n",
    "        \"tm\": (1982, 2012),\n",
    "        \"oli-tirs\": (2013, 2023),\n",
    "        \"etm\": (1999, 2022)}[sensor_name]\n",
    "    if start_year <= year <= end_year:\n",
    "        return \"available\"\n",
    "    return \"unavailable\"\n",
    "\n",
    "\n",
    "def rio_env() -> rio.Env:\n",
    "    return rio.Env(rio.session.AWSSession(boto3.Session(), requester_pays=True),\n",
    "                 GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n",
    "                 GDAL_HTTP_MAX_RETRY=30,\n",
    "                 GDAL_HTTP_RETRY_DELAY=random.randint(3, 15))\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     log.info('Initializing filesystem')\n",
    "#     # storage_options = {'profile': \"ceph\",\n",
    "#     #                    'client_kwargs': {\"endpoint_url\": ENDPOINT}}\n",
    "    \n",
    "#     storage_options = {'requester_pays': True}\n",
    "    \n",
    "#     fs = fsspec.filesystem(\"s3\", **storage_options)\n",
    "        \n",
    "#     start = dt.datetime.strptime('20130501', '%Y%m%d').date()\n",
    "#     end = dt.datetime.strptime('20130901', '%Y%m%d').date()\n",
    "\n",
    "#     log.info('Identifying inputs')\n",
    "#     deets = []\n",
    "#     for sensor in LANDSAT_SENSORS:\n",
    "#         if check_sensor_availability(sensor, start.year) == 'available':\n",
    "#             deets.extend(find_observations(fs, start, end, sensor, 'CU', 3, 10))\n",
    "        \n",
    "#     # windows = [Window(col_off=x, row_off=y, width=100, height=100)\n",
    "#     #            for x in range(0, 5000, 100)\n",
    "#     #            for y in range(0, 5000, 100)]\n",
    "    \n",
    "#     windows = [Window(col_off=0, row_off=0, width=100, height=100)]\n",
    "    \n",
    "#     func = partial(comp_worker_pure_rio,\n",
    "#                    ids=deets,\n",
    "#                    storage_options=storage_options)\n",
    "\n",
    "#     # with mp.Pool(processes=4) as pool:\n",
    "#     #     log.info(f'Begin processing')\n",
    "#     #     for window, arrs in pool.imap_unordered(func, windows):\n",
    "#     #         log.info('Processed...')\n",
    "\n",
    "#     log.info(f'Begin processing')\n",
    "#     for window, arrs in map(func, windows):\n",
    "#         log.info('Processed...')\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     mp.set_start_method('forkserver')\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ef988-1aa1-4a75-a929-a3477d0732d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "# Get the current date and time\n",
    "start_time = dt.datetime.now()\n",
    "formatted_datetime = start_time.strftime('%Y%m%d_%H%M')\n",
    "\n",
    "#%=====================USER INPUT========================\n",
    "start = dt.datetime.strptime('19840508', '%Y%m%d').date()\n",
    "end = dt.datetime.strptime('19840930', '%Y%m%d').date()\n",
    "\n",
    "horiz = 4 # horizontal location\n",
    "vert =  2 # vertical location\n",
    "\n",
    "max_x = 1000\n",
    "max_y = 5000\n",
    "interval = 100\n",
    "\n",
    "start_x = 0\n",
    "start_y = 0\n",
    "\n",
    "windows = [Window(col_off=x, row_off=y, width=interval, height=interval)\n",
    "           for x in range(start_x, max_x, interval)\n",
    "           for y in range(start_y, max_y, interval)]\n",
    "\n",
    "n_qa_filters = 5\n",
    "n_bands = 6\n",
    "\n",
    "\n",
    "#%%%===================Setup Logging Output==============================\n",
    "log.info(f'Initializing filesystem {start_time}')\n",
    "# storage_options = {'profile': \"ceph\",\n",
    "#                    'client_kwargs': {\"endpoint_url\": ENDPOINT}}\n",
    "\n",
    "storage_options = {'requester_pays': True}\n",
    "fs = fsspec.filesystem(\"s3\", **storage_options)\n",
    "\n",
    "log.info('Identifying inputs')\n",
    "deets = []\n",
    "for sensor in LANDSAT_SENSORS:\n",
    "    # print(f'sensor: {sensor}')\n",
    "    if check_sensor_availability(sensor, start.year) == 'available':\n",
    "        print(f'sensor: {sensor} = available')\n",
    "        deets.extend(find_observations(fs, start, end, sensor, 'CU', horiz, vert))\n",
    "log.info(f'Observations found: {len(deets)}')\n",
    "\n",
    "# if len(deets) < 10:\n",
    "#     log.info(f'Adding extra year to observation list...')\n",
    "#     # Increment both start and end dates by one year\n",
    "#     start += timedelta(days=365)\n",
    "#     end += timedelta(days=365)\n",
    "#     log.info(f'New start and end dates: {start}:{end}')\n",
    "#     for sensor in LANDSAT_SENSORS:\n",
    "#         if check_sensor_availability(sensor, start.year) == 'available':\n",
    "#             deets.extend(find_observations(fs, start, end, sensor, 'CU', horiz, vert))\n",
    "#     log.info(f'New total Observations found: {len(deets)}')\n",
    "        \n",
    "\n",
    "func = partial(comp_worker_pure_rio,\n",
    "               ids=deets,\n",
    "               storage_options=storage_options)\n",
    "\n",
    "\n",
    "# # Create a larger array filled with zeros\n",
    "# out_comp_arr = np.zeros((6, max_x, max_y), dtype=int)\n",
    "# for window, window_arr in map(func, windows):\n",
    "#     for i in range(window_arr.shape[0]):\n",
    "#         out_comp_arr[i][window.row_off:window.row_off + window.height, window.col_off:window.col_off + window.width] = window_arr[i]\n",
    "#     log.info('Processed...')\n",
    "#     log.info('\\n\\n')\n",
    "\n",
    "\n",
    "#%%===================SET OUT FILE NAMES==============================\n",
    "# create out tif name with current date time\n",
    "out_comp_tif = f'comp_{horiz:03d}{vert:03d}_{start.year}_{max_x}xy{interval}i_{formatted_datetime}.tif'\n",
    "out_qa_tif = f'QA_{horiz:03d}{vert:03d}_{start.year}_{len(deets)}_{max_x}xy{interval}i_{formatted_datetime}.tif'\n",
    "\n",
    "\n",
    "#%%==================Retrieve metadata from a single input obs=======================\n",
    "# The whole window since the final tif will be the whole window\n",
    "cog_metadata = get_cog_metadata(Window(col_off=0, row_off=0, width=max_x, height=max_y), deets)\n",
    "transform = cog_metadata[0]['transform']\n",
    "dtype = cog_metadata[0]['dtype']\n",
    "crs = cog_metadata[0]['crs']\n",
    "\n",
    "log.info(f'BEGIN PROCESSING: {out_comp_tif}')\n",
    "\n",
    "\n",
    "\n",
    "#%%====================FILL COMPOSITE and QA RASTER==================================\n",
    "\n",
    "# # Open the output datasets outside the loop\n",
    "# with rio.open(out_comp_tif, 'w', driver='GTiff', nodata=None, width=max_x, height=max_y,\n",
    "#               count=6, dtype=dtype, crs=crs, transform=transform) as out_comp_dataset, \\\n",
    "#      rio.open(out_qa_tif, 'w', driver='GTiff', nodata=None, width=max_x, height=max_y,\n",
    "#               count=n_qa_filters, dtype=dtype, crs=crs, transform=transform) as out_qa_dataset:\n",
    "\n",
    "#     for window in windows:\n",
    "#         comp_window_arr = func(window)  # func returns a tuple, thus comp_window_arr[1] is used below\n",
    "\n",
    "#         # Write the comp array\n",
    "#         out_comp_dataset.write(comp_window_arr[1], window=((window.row_off, window.row_off + window.height),\n",
    "#                                                            (window.col_off, window.col_off + window.width)))\n",
    "\n",
    "#         # Write the QA array\n",
    "#         out_qa_dataset.write(np.array(comp_window_arr[2]), window=((window.row_off, window.row_off + window.height),\n",
    "#                                                                    (window.col_off, window.col_off + window.width)))\n",
    "\n",
    "#         log.info(f'PROCESSED WINDOW: {window} \\n\\n')\n",
    "\n",
    "\n",
    "# #%%===============================FILL COMPOSITE RASTER w/ Multiprocessing Pool========================\n",
    "def process_window(window):\n",
    "    comp_window_arr = func(window)\n",
    "    return comp_window_arr, window\n",
    "\n",
    "# Number of CPU cores\n",
    "num_cores = mp.cpu_count()\n",
    "\n",
    "with rio.open(out_comp_tif, 'w', driver='GTiff', nodata=None, width=max_x, height=max_y,\n",
    "              count=6, dtype=dtype, crs=crs, transform=transform) as out_comp_dataset, \\\n",
    "     rio.open(out_qa_tif, 'w', driver='GTiff', nodata=None, width=max_x, height=max_y,\n",
    "              count=n_qa_filters, dtype=dtype, crs=crs, transform=transform) as out_qa_dataset:\n",
    "\n",
    "    with mp.Pool(num_cores) as pool:\n",
    "        results = pool.map(process_window, windows)\n",
    "\n",
    "    for comp_window_arr, window in results:\n",
    "        # Write the comp array\n",
    "        out_comp_dataset.write(comp_window_arr[1], window=((window.row_off, window.row_off + window.height),\n",
    "                                                           (window.col_off, window.col_off + window.width)))\n",
    "\n",
    "        # Write the QA array\n",
    "        out_qa_dataset.write(np.array(comp_window_arr[2]), window=((window.row_off, window.row_off + window.height),\n",
    "                                                                   (window.col_off, window.col_off + window.width)))\n",
    "\n",
    "\n",
    "        log.info(f'PROCESSED WINDOW: {window} \\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Record the end time and calculate the elapsed time\n",
    "elapsed_time = dt.datetime.now() - start_time\n",
    "\n",
    "# Convert elapsed time to seconds\n",
    "elapsed_seconds = elapsed_time.total_seconds()\n",
    "\n",
    "# Print the result in minutes or hours\n",
    "if elapsed_time.total_seconds() < 60:\n",
    "    log.info(f\"Processing time: {elapsed_time.total_seconds():.2f} seconds\")\n",
    "elif elapsed_time.total_seconds() < 3600:\n",
    "    log.info(f\"Processing time: {elapsed_time.total_seconds()/60:.2f} minutes\")\n",
    "else:\n",
    "    log.info(f\"Processing time: {elapsed_time.total_seconds()/3600:.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7226463-f7a8-4fac-af80-aa0d126ed335",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_ids = ['B3', 'B2', 'B1']\n",
    "b = 'LT05_CU_003010_19840513_20210421_02_SR_B1.TIF'\n",
    "# # Check if any of the band_ids is present in the string b\n",
    "# if any(band_id in b for band_id in band_ids):\n",
    "#     print('yes')\n",
    "        \n",
    "for idx3, band_id in enumerate(band_ids):\n",
    "    if band_id in b:\n",
    "        print(f'>>> b: {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec504b-8089-4266-aa49-22511765dda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3a59e06-a418-4296-9ed7-f529b87d2711",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d087757-894a-4fde-9879-d120d8e84694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 nlcd_compositing_TR.py >> output_014016_1984_1985_5000xy100i_20231219_2134.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b0df2-119f-4265-8fd3-3fbb9ef6e0e2",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc3b27-cfa1-4a83-88aa-1d86ca4cd327",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plotting Composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358704d9-b457-4c6f-9e8e-2e09024bd993",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_options = {'requester_pays': True}\n",
    "fs = fsspec.filesystem(\"s3\", **storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b777a5-8d40-4ba9-9e74-41d15c102676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fs.ls('usgs-landsat-ard/collection02/tm/1984/CU/014/016/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd54002-77e6-45ee-a908-4a2cf87d1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ARD_composite(ARDs, bandcombotype: str):\n",
    "    \"\"\"\n",
    "    Retrieve red, green and blue tiles for select ARDs then plot the rgb composites for each.\n",
    "    \n",
    "    PARAMETERS:\n",
    "        ARDs (list) - a list of ARDs with parameters set by the find_observations function\n",
    "    \"\"\"\n",
    "    log.info(f'Number of Observations Found: {len(ARDs)}')\n",
    "    \n",
    "    env = rio_env()\n",
    "    \n",
    "\n",
    "    for d in ARDs:\n",
    "        log.info(f'Pulling data for {d}')\n",
    "        bands_arr = np.zeros(shape=(3, 5000, 5000), dtype=int)\n",
    "        layers = sr_std_layers(d)\n",
    "        idx = 0    \n",
    "        \n",
    "        band_ids = get_band_ids('rgb', d.sensor)\n",
    "        \n",
    "        for band_id in band_ids:\n",
    "            # print(f'band_id: {band_id}')\n",
    "            for b in layers: \n",
    "                if band_id in b:\n",
    "                    bands_arr[idx] = read_raster_pure_rio('/'.join([d.std_path, b]), env, boundless=True)\n",
    "                    idx = idx+1\n",
    "\n",
    "        # Initialize an empty list to store the RGB bands\n",
    "        scaled_bands = []\n",
    "        composite = []\n",
    "        \n",
    "        # Loop through each band data and transform\n",
    "        for band_array in bands_arr:\n",
    "            # Append the band data after scaling to 0-1 range\n",
    "            scaled_bands.append((band_array / band_array.max()).astype(np.float32))\n",
    "\n",
    "        # Create a natural color composite using the RGB bands\n",
    "        composite = np.stack(scaled_bands, axis=-1)\n",
    "\n",
    "        # Display the natural color composite\n",
    "        plt.imshow(composite)\n",
    "        plt.title(f'{bandcombotype} composite:  {d.root_id} ')\n",
    "        plt.show()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfdc8e6-9509-4923-92e4-871029073dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARDs = find_observations(fs, dt.date(1984, 5, 1), dt.date(1984, 9, 30), 'tm', 'CU', '003', '003')\n",
    "plot_ARD_composite(ARDs, 'rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f58cd3-20d8-47db-86a4-74b7eadf76fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Not in Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3daa86-651c-4acc-b793-3824353e869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_count_arrays = [np.array([[4, 4, 5, 5, 5],\n",
    "#        [4, 4, 4, 4, 5],\n",
    "#        [5, 4, 4, 4, 4],\n",
    "#        [5, 5, 5, 4, 4],\n",
    "#        [5, 5, 5, 5, 5]]), np.array([[0, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0]]), np.array([[1, 1, 1, 1, 1],\n",
    "#        [1, 1, 1, 1, 1],\n",
    "#        [0, 1, 1, 1, 1],\n",
    "#        [0, 0, 0, 1, 1],\n",
    "#        [0, 0, 0, 0, 0]]), np.array([[0, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0]]), np.array([[0, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f344a-4389-44c6-becd-de1aa62fa2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with rio.open('out_qa1.tif', 'w', driver='GTiff', nodata=None, width=max_x, height=max_y,\n",
    "#           count=len(qa_count_arrays), dtype=dtype, crs=crs, transform=transform) as out_dataset:\n",
    "\n",
    "#     print(f'window: {window}')\n",
    "#     # Write the entire 3D array at once\n",
    "#     out_dataset.write(np.array(qa_count_arrays), window=((window.row_off, window.row_off + window.height),\n",
    "#                                               (window.col_off, window.col_off + window.width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c36c48-b954-4efc-81ee-cb902ae08d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'TYPE: {type(comp_window_arr[1])}')\n",
    "\n",
    "# type(np.array(qa_count_arrays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78175edb-c735-4337-9b87-34c2d94f33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# # Save the current system output (stdout)\n",
    "# original_stdout = sys.stdout\n",
    "\n",
    "# # Specify the file where you want to redirect the output\n",
    "# output_file_path = 'output.txt'\n",
    "\n",
    "# # Open the file in write mode\n",
    "# with open(output_file_path, 'w') as file:\n",
    "#     # Redirect the system output to the file\n",
    "#     sys.stdout = file\n",
    "    \n",
    "#     # Now, any print statements will be written to the file\n",
    "#     print('Hello, World!')\n",
    "#     print('This is a system output.')\n",
    "\n",
    "# # Restore the original system output\n",
    "# sys.stdout = original_stdout\n",
    "\n",
    "# # File has been closed automatically due to the 'with' statement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b96ade-6d57-4ce0-bd60-ba897b8539fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boo_arr = [np.array([[[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]],\n",
    "\n",
    "#        [[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]],\n",
    "\n",
    "#        [[ True,  True,  True,  True,  True],\n",
    "#         [ True,  True,  True,  True,  True],\n",
    "#         [ True,  True,  True,  True,  True],\n",
    "#         [ True,  True,  True,  True,  True],\n",
    "#         [ True,  True,  True,  True,  True]]]), np.array([[[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]],\n",
    "\n",
    "#        [[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]],\n",
    "\n",
    "#        [[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]]]), np.array([[[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]],\n",
    "\n",
    "#        [[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]],\n",
    "\n",
    "#        [[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]]]), np.array([[[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]],\n",
    "\n",
    "#        [[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]],\n",
    "\n",
    "#        [[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]]]), np.array([[[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]],\n",
    "\n",
    "#        [[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]],\n",
    "\n",
    "#        [[False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False],\n",
    "#         [False, False, False, False, False]]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b3c36-a232-45af-8286-7114f68fc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.bitwise_or.reduce(boo_arr, axis=1) #this may tell us that the qa_fill had more of an influence on the std_mask results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba71dd-23f3-489e-ade6-ecef5ac15510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_qa_arrays = [arr.astype(int) for arr in boo_arr]\n",
    "\n",
    "# # Assuming comp_window_arr.shape[0] gives the correct number of bands\n",
    "# with rio.open(out_qa_tif, 'w', driver='GTiff', nodata=None, width=max_x, height=max_y,\n",
    "#               count=len(binary_qa_arrays), dtype=dtype, crs=crs, transform=transform) as out_qa_tif:\n",
    "  \n",
    "#     for window in windows:\n",
    "        \n",
    "#         # Write the entire 3D array at once\n",
    "#         out_qa_tif.write(comp_window_arr[1], window=((window.row_off, window.row_off + window.height),\n",
    "#                                                   (window.col_off, window.col_off + window.width)))\n",
    "        \n",
    "#         log.info(f'PROCESSED WINDOW: {window} \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f12bde-b847-490d-941f-f9983eea6104",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Fill Composite Xarray**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a910a-31c9-4975-9b63-fc4fd881c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the band numbers\n",
    "# band_numbers = sr_bandnumbers(sensor)\n",
    "\n",
    "# # Create an empty xarray dataset\n",
    "# comp_xarr = xr.Dataset()\n",
    "\n",
    "# # Set coordinate values for 'x' and 'y' based on the shape of 'out_comp_arr'\n",
    "# comp_xarr['x'] = np.arange(out_comp_arr.shape[2])\n",
    "# comp_xarr['y'] = np.arange(out_comp_arr.shape[1])\n",
    "\n",
    "# # Iterate through the bands and add them to the dataset\n",
    "# for band_number, band_data in zip(band_numbers, out_comp_arr):\n",
    "#     data_var_name = f'band_{band_number}'\n",
    "#     comp_xarr[data_var_name] = (['y', 'x'], band_data)  # Note the order of dimensions\n",
    "\n",
    "# # Optionally, set metadata or attributes for the dataset or data variables\n",
    "# comp_xarr.attrs['description'] = (f'{sensor} Landsat Sensor Bands')\n",
    "\n",
    "# # Display the resulting xarray dataset\n",
    "# print(comp_xarr)\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# comp_xarr[[\"band_3\", \"band_2\", \"band_1\"]].to_array().plot.imshow(robust=True, ax=ax)\n",
    "# ax.set_title(\"Composite\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a0fb7-8f4d-4d37-8de6-88a7f56b24f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !find ~/ -name \"lsc2ard.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e58e5-c1fb-4d7e-8bb5-7c8404852785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# # Source directory: lcnext-core/lcnext\n",
    "# source_directory = 'lcnext-core/lcnext'\n",
    "\n",
    "# # Destination directory: lcnext-trr\n",
    "# destination_directory = 'lcnext_trr'\n",
    "\n",
    "# # Copy everything from the source to the destination\n",
    "# shutil.copytree(source_directory, destination_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab194a1a-018d-4641-a537-3ffd393e9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 /home/jovyan/delete_all.py -d '/home/jovyan/LCNEXT/lcnext-trr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795194d-2f2b-48e2-9909-7dc07b042f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids=deets\n",
    "\n",
    "# env = rio_env()\n",
    "\n",
    "# big_arr = np.zeros(shape=(6, len(ids), 5000, 5000), dtype=int) \n",
    "\n",
    "# for idx1, d in enumerate(ids):\n",
    "#     log.info(f'Pulling data for {d}')\n",
    "#     layers = sr_std_layers(d)\n",
    "\n",
    "#     for idx2, b in enumerate(layers):\n",
    "#         log.info(f'Retrieving layer {b}')\n",
    "#         big_arr[idx2, idx1] = read_raster_pure_rio('/'.join([d.std_path, b]), env, boundless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e68ba14-5810-4223-a038-c1161b0d6940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shapely.geometry import box\n",
    "# def read_bounds(path: str, env: rio.env, index=1, window=None, boundless=False, fill_value=None):\n",
    "#     try:\n",
    "#         with env:\n",
    "#             with rio.open('s3://' + path) as src:\n",
    "#                 bounds = src.bounds\n",
    "#                 outline_polygon = box(bounds.left, bounds.bottom, bounds.right, bounds.top)\n",
    "#                 return gp.GeoDataFrame(geometry=[outline_polygon], crs=src.crs)\n",
    "#     except:\n",
    "#         print(path)\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4897ec0-ec59-487d-b83e-3f6fbbc07af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rasterio\n",
    "\n",
    "# def get_tif_extents(tif_path):\n",
    "#     with rasterio.open(tif_path) as src:\n",
    "#         bounds = src.bounds\n",
    "#     return bounds\n",
    "\n",
    "# # Replace 'your_file.tif' with the actual path to your GeoTIFF file\n",
    "# tif_extents = get_tif_extents('your_file.tif')\n",
    "# print(f'The extents of the GeoTIFF file are: {tif_extents}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472cd15c-f9e6-44c1-928b-0a21ef5c64bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounds = []\n",
    "\n",
    "# env = rio_env()\n",
    "\n",
    "# for d in deets:\n",
    "#     log.info(f'Pulling data for {d}')\n",
    "#     layers = sr_std_layers(d)\n",
    "    \n",
    "#     b = layers[0]\n",
    "    \n",
    "#     log.info(f'Retrieving bounds from first layer {b}')\n",
    "#     bounds.append(read_bounds('/'.join([d.std_path, b]), env, boundless=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eddbc3-43cf-4829-b0bf-0ff1ca22111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gp\n",
    "\n",
    "# # Specify the path to the shapefile\n",
    "# shapefile_path = 'CONUS_ARD_shapefile/conus_c2_ard_grid.shp'\n",
    "\n",
    "# # Open the shapefile using GeoPandas\n",
    "# gdf = gp.read_file(shapefile_path)\n",
    "\n",
    "# # Display basic information about the GeoDataFrame\n",
    "# print(\"Shape of the GeoDataFrame:\", gdf.shape)\n",
    "# print(\"\\nColumns in the GeoDataFrame:\", gdf.columns)\n",
    "\n",
    "# # Display the GeoDataFrame\n",
    "# print(\"\\nFirst few rows of the GeoDataFrame:\")\n",
    "# print(gdf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc4d17-34c0-43a4-abcd-a653925547da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf[(gdf['h'] == horiz) & (gdf['v'] == vert)]['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60358cc0-7741-40df-8c73-3d723c2fbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "# from folium.plugins import MarkerCluster\n",
    "# from shapely.geometry import mapping\n",
    "\n",
    "# # Define an HTML color code (e.g., blue color)\n",
    "# aoi_color = \"#0489B1\"\n",
    "# scene_color = \"#A4A4A4\"\n",
    "\n",
    "# gdf = gdf.to_crs(epsg=4326)  # Convert to WGS84 (latitude, longitude)\n",
    "\n",
    "# # Filter the GeoDataFrame based on h and v values\n",
    "# filtered_gdf = gdf[(gdf['h'] == horiz) & (gdf['v'] == vert)]\n",
    "\n",
    "# # Create a Folium map centered at the mean of the filtered geometries\n",
    "# center_lat, center_lon = filtered_gdf['geometry'].centroid.y.mean(), filtered_gdf['geometry'].centroid.x.mean()\n",
    "# m = folium.Map(location=[center_lat, center_lon], zoom_start=8)\n",
    "\n",
    "# # Create a MarkerCluster layer for better visualization (optional)\n",
    "# marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "# # Iterate over filtered GeoDataFrame rows and add each geometry to the map\n",
    "# for idx, row in filtered_gdf.iterrows():\n",
    "#     geojson_data = mapping(row['geometry'])\n",
    "#     folium.GeoJson(geojson_data, \n",
    "#                    tooltip=f\"h: {row['h']},v: {row['v']}\", \n",
    "#                    style_function=lambda x: {\n",
    "#                        \"fillColor\": aoi_color,  # Change the fill color to blue\n",
    "#                        \"color\" : aoi_color\n",
    "#                    }\n",
    "#                   ).add_to(marker_cluster)\n",
    "\n",
    "\n",
    "# # Add extent of scenes\n",
    "# for b in bounds:\n",
    "#     b = b.to_crs(epsg=4326)  # Convert to WGS84 (latitude, longitude)\n",
    "#     folium.GeoJson(\n",
    "#         b['geometry'],\n",
    "#         name=\"Additional Polygon\",\n",
    "#         style_function=lambda feature: {\n",
    "#             \"fillColor\": None,  # NO fill\n",
    "#             \"color\": scene_color,  # Change the border color to black\n",
    "#             \"weight\" : 1, # border line thickness\n",
    "#         },\n",
    "#     ).add_to(m)\n",
    "    \n",
    "\n",
    "# # Display the map\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60545402-ec97-4415-9edc-0ebf7a6b969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Record the start time\n",
    "#start_time = datetime.now()\n",
    "\n",
    "# Your code to be measured for processing time\n",
    "# ...\n",
    "\n",
    "# # Record the end time and calculate the elapsed time\n",
    "# elapsed_time = datetime.now() - start_time\n",
    "\n",
    "# # Convert elapsed time to seconds\n",
    "# elapsed_seconds = elapsed_time.total_seconds()\n",
    "\n",
    "# # Print the result in minutes or hours\n",
    "# if elapsed_time.total_seconds() < 60:\n",
    "#     print(f\"Processing time: {elapsed_time.total_seconds():.2f} seconds\")\n",
    "# elif elapsed_time.total_seconds() < 3600:\n",
    "#     print(f\"Processing time: {elapsed_time.total_seconds()/60:.2f} minutes\")\n",
    "# else:\n",
    "#     print(f\"Processing time: {elapsed_time.total_seconds()/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0fa93a-1b22-4ea2-96a8-3006ea3686d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def calculate_qa_percent_true(qa_arrays: np.ndarray, ids) -> np.ndarray:\n",
    "  \n",
    "#     \"\"\" \n",
    "#     The calculate_qa_percent_true script is designed to calculate the percentage of true pixel values \n",
    "#     within each observation (ARD tiles stored as matrices) and grouped in by assessment (QA) layers. The QA layers, \n",
    "#     including parameters such as fill, cirrus, cloud, cloud shadow, and snow, are derived from the lsc2ard library. \n",
    "#     The script computes the percentage of true values for each array for each observation within these QA layers and organizes the results \n",
    "#     into a dictionary, providing key-wise lists of indices and corresponding percentages.\n",
    "    \n",
    "#     Input is a dictionary of qa_arrays using the repective qa_filter to fill in values from lsc2ard imported as c2ard in this case\n",
    "#             e.g: qa_arrays = {\n",
    "#             'qa_fill': c2ard.qa_fill(qa_arr),\n",
    "#             'qa_cirrus': c2ard.qa_cirrus(qa_arr),\n",
    "#             'qa_cloud': c2ard.qa_cloud(qa_arr),\n",
    "#             'qa_cl_shadow': c2ard.qa_cl_shadow(qa_arr),\n",
    "#             'qa_snow': c2ard.qa_snow(qa_arr)\n",
    "#             }\n",
    "            \n",
    "#     Example result:\n",
    "#             {'qa_fill': [{'index': 0, 'percent_true': 0.0},\n",
    "#           {'index': 1, 'percent_true': 100.0},\n",
    "#           {'index': 2, 'percent_true': 0.0}],\n",
    "#          'qa_cirrus': [{'index': 0, 'percent_true': 0.0},\n",
    "#           {'index': 1, 'percent_true': 0.0},\n",
    "#           {'index': 2, 'percent_true': 0.0}],\n",
    "#          'qa_cloud': [{'index': 0, 'percent_true': 0.0},\n",
    "#           {'index': 1, 'percent_true': 0.0},\n",
    "#           {'index': 2, 'percent_true': 0.0}],\n",
    "#          'qa_cl_shadow': [{'index': 0, 'percent_true': 0.0},\n",
    "#           {'index': 1, 'percent_true': 0.0},\n",
    "#           {'index': 2, 'percent_true': 0.0}],\n",
    "#          'qa_snow': [{'index': 0, 'percent_true': 0.0},\n",
    "#           {'index': 1, 'percent_true': 0.0},\n",
    "#           {'index': 2, 'percent_true': 0.0}]}\n",
    "      \n",
    "#       Where the qa_fill is 100% TRUE for pixels in the observation at index '1' (2nd ARD tile)\n",
    "#     \"\"\"\n",
    "#     # New dictionary to store results for each key\n",
    "#     results_dict = {}\n",
    "    \n",
    "\n",
    "#     # Loop through the dictionary\n",
    "#     for key, qa_array in qa_arrays.items():\n",
    "#         print(f\"{key.upper()}:\")\n",
    "            \n",
    "#         # Initialize an empty list to store the results\n",
    "#         results = []\n",
    "\n",
    "#         # Loop through the collection of arrays\n",
    "#         for idx, qa_arr in enumerate(qa_array):\n",
    "            \n",
    "#             # Calculate the percentage of true values\n",
    "#             percent_true = (np.sum(qa_arr) / qa_arr.size) * 100\n",
    "            \n",
    "#             if percent_true > 0:\n",
    "#                 print(f\"    {ids[idx].root_id}: {percent_true:.2f}%\")\n",
    "            \n",
    "#             # Append the results to the list\n",
    "#             results.append({\n",
    "#                 'index': idx,\n",
    "#                 'root_id' : ids[idx].root_id,\n",
    "#                 'percent_true': percent_true\n",
    "#             })\n",
    "            \n",
    "#         results_dict[key] = results;\n",
    "\n",
    "#     return results_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9720d73-a43d-461d-b7a2-403d33d9e201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
